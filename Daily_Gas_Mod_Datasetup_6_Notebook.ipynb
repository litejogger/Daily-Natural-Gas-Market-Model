{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "# set file path to find input parameter data to write to model file from inputs\n",
    "input_data_filepath = r'C:\\Users\\calisy\\OneDrive\\Energy Research\\NG Project\\code\\Gas Model\\Model Data Inputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files Referenced\n",
    "\n",
    "1. QP_BASE_NA_Daily_Downscaled.csv\n",
    "2. PipeCapacities_Dec22_Daily_Downscaled.csv\n",
    "3. imports - daily pipeline and LNG smoothed - 2015-2019.csv\n",
    "4. exports - daily pipeline and LNG smoothed - 2015-2019.csv\n",
    "5. state_daily_production_demand_gulf_added_QPS_2015-2019-bcf.csv\n",
    "6. PipelineTariff.csv\n",
    "7. PipeTariffCurveQty_Daily_Downscaled.csv\n",
    "8. state_max_daily_storage.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Simulation Horizon and production steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimDays = 372 # add 7 for first week of data from 2018\n",
    "HorizonDays = 7  ##planning horizon \n",
    "\n",
    "\n",
    "data_name = 'daily_gas_mod_data_6_base' # base means does not include storage costs, calculated separately\n",
    "\n",
    "\n",
    "#### set number of supply curve steps ####\n",
    "\n",
    "# set num steps\n",
    "num_steps = 20\n",
    "\n",
    "# set num additional steps between 5 and 6 for adding additional Qsteps\n",
    "k = num_steps - 6\n",
    "\n",
    "pipeline_slack_cost = 1000\n",
    "production_slack_cost = 5000\n",
    "storage_slack_cost = 7000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script to add additional supply curve steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qbase to new Qsteps dataframe complete\n"
     ]
    }
   ],
   "source": [
    "#read parameters for QP supply regions\n",
    "df_QP_NA = pd.read_csv(input_data_filepath + '/' + 'QP_BASE_NA_Daily_Downscaled.csv',header=0)\n",
    "\n",
    "###### add supply curve steps based on setting above #######\n",
    "df_qbase = df_QP_NA.copy()\n",
    "\n",
    "# column for quantity between steps to be added as 'x' values\n",
    "df_qbase['quantity_added'] = (df_qbase['Qstep6'] - df_qbase['Qstep5']) / (k + 1)\n",
    "\n",
    "# rename 6 for new naming based on number of steps specified\n",
    "last_Qstep = 'Qstep'+str(num_steps)\n",
    "last_Pstep = 'Pstep'+str(num_steps)\n",
    "\n",
    "df_qbase = df_qbase.rename(columns={'Qstep6':last_Qstep,'Pstep6':last_Pstep})\n",
    "\n",
    "# print()\n",
    "# print(df_qbase)\n",
    "\n",
    "\n",
    "\n",
    "# loop to create new qsteps and psteps via linear interpolation \n",
    "\n",
    "for i in range(1,k+1):\n",
    "    df_qbase['Qstep'+str(5+i)] = df_qbase['Qstep5'] + i*df_qbase['quantity_added']\n",
    "    \n",
    "    df_qbase['Pstep'+str(5+i)] = df_qbase['Pstep5'] + (df_qbase['Qstep'+str(5+i)]- df_qbase['Qstep5'])*((df_qbase[last_Pstep]-df_qbase['Pstep5'])/(df_qbase[last_Qstep]-df_qbase['Qstep5']))\n",
    "\n",
    "# change column order to match original\n",
    "newq_cols = []\n",
    "[newq_cols.append('Qstep'+str(i)) for i in range(1,num_steps+1)]\n",
    "\n",
    "newp_cols = []\n",
    "[newp_cols.append('Pstep'+str(i)) for i in range(1,num_steps+1)]\n",
    "\n",
    "\n",
    "cols = ['producer','node'] + newq_cols + newp_cols\n",
    "\n",
    "\n",
    "df_qbase = df_qbase[cols]\n",
    "\n",
    "\n",
    "# save for comparison later\n",
    "df_qbase.to_csv(input_data_filepath + '/' + 'QP_BASE_NA_Daily_Downscaled_'+str(num_steps)+'_steps.csv',index=False)\n",
    "\n",
    "# change back df name\n",
    "df_QP_NA = df_qbase.copy()\n",
    "\n",
    "\n",
    "\n",
    "# rename qps producer column\n",
    "qps = list(df_QP_NA['producer'])\n",
    "for q in qps:\n",
    "    new_q = q + '_qps'\n",
    "    i = qps.index(q)\n",
    "    qps[i] = new_q\n",
    "df_QP_NA['producer'] = qps\n",
    "\n",
    "print('Qbase to new Qsteps dataframe complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create line capacities dataframe and the line-to-node matrix \n",
    "\n",
    "matrix (-1 leaving node, +1 entering node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line to node matrix complete\n",
      "QPS to node matrix complete\n"
     ]
    }
   ],
   "source": [
    "#read downscaled pipeline data\n",
    "df_line_params = pd.read_csv(input_data_filepath + '/' + 'PipeCapacities_Dec22_Daily_Downscaled.csv',header=0,index_col=0) # I CHANGED THIS TO BE ORIGINAL SO EFFECTIVELY NO REAL CAPACITY\n",
    "i_nodes = list(df_line_params.index)\n",
    "c_nodes = list(df_line_params.columns)\n",
    "nodes = i_nodes + c_nodes\n",
    "all_nodes = [i for n, i in enumerate(nodes) if i not in nodes[:n]]\n",
    "\n",
    " \n",
    "lines = []\n",
    "caps = []\n",
    "tos = []\n",
    "froms = []\n",
    "\n",
    "for i in all_nodes:\n",
    "    for j in all_nodes:\n",
    "        if i != j:\n",
    "            line = i+'_to_'+j\n",
    "            if df_line_params.loc[i,j]>0:\n",
    "                froms.append(i.replace('-','_'))\n",
    "                tos.append(j.replace('-','_'))\n",
    "                lines.append(line)\n",
    "                caps.append(df_line_params.loc[i,j])\n",
    "                \n",
    "for l in lines:\n",
    "    new_l = l.replace('-','_')\n",
    "    i = lines.index(l)\n",
    "    lines[i] = new_l\n",
    "\n",
    "df_pipelines = pd.DataFrame(caps,index = lines,columns = ['capacity'])\n",
    "\n",
    "\n",
    "# fix the change from '-' to '_'\n",
    "# must be done after above since the dataframe has old names\n",
    "full_nodes_list = [i for n, i in enumerate(nodes) if i not in nodes[:n]]\n",
    "all_nodes = []\n",
    "for l in full_nodes_list:\n",
    "    all_nodes.append(l.replace('-','_'))\n",
    "\n",
    "# print(all_nodes)\n",
    "\n",
    "\n",
    "# create line to node matrix\n",
    "# l_to_n = np.zeros((len(lines),len(all_nodes)))\n",
    "\n",
    "df_l_to_n = pd.DataFrame(columns=all_nodes)\n",
    "df_l_to_n['line'] = lines\n",
    "df_l_to_n.set_index('line',inplace=True)\n",
    "\n",
    "for i in range(0,len(lines)):\n",
    "    f = froms[i]\n",
    "    t = tos[i]\n",
    "    df_l_to_n.loc[lines[i],f] = -1\n",
    "    df_l_to_n.loc[lines[i],t] = 1\n",
    "\n",
    "df_l_to_n = df_l_to_n.fillna(0)\n",
    "\n",
    "df_l_to_n.to_csv(input_data_filepath + '/' + 'line_to_node_map.csv')\n",
    "\n",
    "# df_line_to_node_map = pd.read_csv(input_data_filepath + '/' + 'line_to_node_map.csv',header=0)\n",
    "\n",
    "df_line_to_node_map = df_l_to_n\n",
    "\n",
    "print('line to node matrix complete')\n",
    "\n",
    "#%% QPS to node matrix (QPS is rows and nodes are columns - simple unidirectional +1 adjacency matrix)\n",
    "\n",
    "\n",
    "# create QPS to node matrix\n",
    "\n",
    "A = np.zeros((len(df_QP_NA),len(all_nodes)))\n",
    "\n",
    "df_A = pd.DataFrame(A)\n",
    "df_A.columns = all_nodes\n",
    "df_A['name'] = list(df_QP_NA['producer'])\n",
    "df_A.set_index('name',inplace=True)\n",
    "\n",
    "for i in range(0,len(df_QP_NA)):\n",
    "    node = df_QP_NA.loc[i,'node']\n",
    "    g = df_QP_NA.loc[i,'producer']\n",
    "    df_A.loc[g,node] = 1\n",
    "\n",
    "df_A.to_csv(input_data_filepath + '/' + 'qps_to_node_map.csv')\n",
    "\n",
    "df_node_to_producer_map = pd.read_csv(input_data_filepath + '/' + 'qps_to_node_map.csv',header=0)\n",
    "\n",
    "print('QPS to node matrix complete')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demand, Production, Imports, Exports time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero demand nodes manually added: MX, CN, border nodes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>state abbrev</th>\n",
       "      <th>date</th>\n",
       "      <th>AL</th>\n",
       "      <th>AR</th>\n",
       "      <th>AZ</th>\n",
       "      <th>CA</th>\n",
       "      <th>CO</th>\n",
       "      <th>CT</th>\n",
       "      <th>DC</th>\n",
       "      <th>DE</th>\n",
       "      <th>FL</th>\n",
       "      <th>...</th>\n",
       "      <th>SD</th>\n",
       "      <th>TN</th>\n",
       "      <th>TX</th>\n",
       "      <th>UT</th>\n",
       "      <th>VA</th>\n",
       "      <th>VT</th>\n",
       "      <th>WA</th>\n",
       "      <th>WI</th>\n",
       "      <th>WV</th>\n",
       "      <th>WY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-25</td>\n",
       "      <td>0.408122</td>\n",
       "      <td>1.540939</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.549696</td>\n",
       "      <td>5.407832</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.009435</td>\n",
       "      <td>25.650856</td>\n",
       "      <td>0.758768</td>\n",
       "      <td>0.298514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.353837</td>\n",
       "      <td>4.319338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>0.408287</td>\n",
       "      <td>1.541958</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.550157</td>\n",
       "      <td>5.415834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.009508</td>\n",
       "      <td>25.656463</td>\n",
       "      <td>0.758999</td>\n",
       "      <td>0.298752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.360228</td>\n",
       "      <td>4.321106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>0.408453</td>\n",
       "      <td>1.542978</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.550618</td>\n",
       "      <td>5.423836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.009580</td>\n",
       "      <td>25.662069</td>\n",
       "      <td>0.759229</td>\n",
       "      <td>0.298990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.366620</td>\n",
       "      <td>4.322875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>0.408619</td>\n",
       "      <td>1.543998</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.551079</td>\n",
       "      <td>5.431838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.009653</td>\n",
       "      <td>25.667675</td>\n",
       "      <td>0.759460</td>\n",
       "      <td>0.299228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.373012</td>\n",
       "      <td>4.324643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>0.408785</td>\n",
       "      <td>1.545018</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.551540</td>\n",
       "      <td>5.439840</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.009725</td>\n",
       "      <td>25.673282</td>\n",
       "      <td>0.759690</td>\n",
       "      <td>0.299466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.379404</td>\n",
       "      <td>4.326411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>0.358319</td>\n",
       "      <td>1.370699</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.514133</td>\n",
       "      <td>5.754834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.008434</td>\n",
       "      <td>29.940069</td>\n",
       "      <td>0.724079</td>\n",
       "      <td>0.288026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.612334</td>\n",
       "      <td>4.041747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>0.357794</td>\n",
       "      <td>1.371871</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.514525</td>\n",
       "      <td>5.761979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.008444</td>\n",
       "      <td>29.975618</td>\n",
       "      <td>0.724831</td>\n",
       "      <td>0.288291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.621137</td>\n",
       "      <td>4.043871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>0.357270</td>\n",
       "      <td>1.373044</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.514916</td>\n",
       "      <td>5.769124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>30.011167</td>\n",
       "      <td>0.725583</td>\n",
       "      <td>0.288556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.629941</td>\n",
       "      <td>4.045996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>0.356745</td>\n",
       "      <td>1.374216</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.515307</td>\n",
       "      <td>5.776269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.008463</td>\n",
       "      <td>30.046716</td>\n",
       "      <td>0.726335</td>\n",
       "      <td>0.288822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.638745</td>\n",
       "      <td>4.048121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0.356745</td>\n",
       "      <td>1.374216</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.515307</td>\n",
       "      <td>5.776269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.008463</td>\n",
       "      <td>30.046716</td>\n",
       "      <td>0.726335</td>\n",
       "      <td>0.288822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.638745</td>\n",
       "      <td>4.048121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>372 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "state abbrev        date        AL        AR        AZ        CA        CO  \\\n",
       "0             2018-12-25  0.408122  1.540939  0.000031  0.549696  5.407832   \n",
       "1             2018-12-26  0.408287  1.541958  0.000024  0.550157  5.415834   \n",
       "2             2018-12-27  0.408453  1.542978  0.000018  0.550618  5.423836   \n",
       "3             2018-12-28  0.408619  1.543998  0.000012  0.551079  5.431838   \n",
       "4             2018-12-29  0.408785  1.545018  0.000006  0.551540  5.439840   \n",
       "..                   ...       ...       ...       ...       ...       ...   \n",
       "367           2019-12-27  0.358319  1.370699  0.000241  0.514133  5.754834   \n",
       "368           2019-12-28  0.357794  1.371871  0.000240  0.514525  5.761979   \n",
       "369           2019-12-29  0.357270  1.373044  0.000239  0.514916  5.769124   \n",
       "370           2019-12-30  0.356745  1.374216  0.000238  0.515307  5.776269   \n",
       "371           2019-12-31  0.356745  1.374216  0.000238  0.515307  5.776269   \n",
       "\n",
       "state abbrev   CT   DC   DE        FL  ...        SD        TN         TX  \\\n",
       "0             0.0  0.0  0.0  0.002163  ...  0.001188  0.009435  25.650856   \n",
       "1             0.0  0.0  0.0  0.002163  ...  0.001188  0.009508  25.656463   \n",
       "2             0.0  0.0  0.0  0.002163  ...  0.001188  0.009580  25.662069   \n",
       "3             0.0  0.0  0.0  0.002163  ...  0.001188  0.009653  25.667675   \n",
       "4             0.0  0.0  0.0  0.002163  ...  0.001188  0.009725  25.673282   \n",
       "..            ...  ...  ...       ...  ...       ...       ...        ...   \n",
       "367           0.0  0.0  0.0  0.002571  ...  0.001190  0.008434  29.940069   \n",
       "368           0.0  0.0  0.0  0.002653  ...  0.001191  0.008444  29.975618   \n",
       "369           0.0  0.0  0.0  0.002736  ...  0.001192  0.008453  30.011167   \n",
       "370           0.0  0.0  0.0  0.002818  ...  0.001193  0.008463  30.046716   \n",
       "371           0.0  0.0  0.0  0.002818  ...  0.001193  0.008463  30.046716   \n",
       "\n",
       "state abbrev        UT        VA   VT   WA   WI        WV        WY  \n",
       "0             0.758768  0.298514  0.0  0.0  0.0  5.353837  4.319338  \n",
       "1             0.758999  0.298752  0.0  0.0  0.0  5.360228  4.321106  \n",
       "2             0.759229  0.298990  0.0  0.0  0.0  5.366620  4.322875  \n",
       "3             0.759460  0.299228  0.0  0.0  0.0  5.373012  4.324643  \n",
       "4             0.759690  0.299466  0.0  0.0  0.0  5.379404  4.326411  \n",
       "..                 ...       ...  ...  ...  ...       ...       ...  \n",
       "367           0.724079  0.288026  0.0  0.0  0.0  6.612334  4.041747  \n",
       "368           0.724831  0.288291  0.0  0.0  0.0  6.621137  4.043871  \n",
       "369           0.725583  0.288556  0.0  0.0  0.0  6.629941  4.045996  \n",
       "370           0.726335  0.288822  0.0  0.0  0.0  6.638745  4.048121  \n",
       "371           0.726335  0.288822  0.0  0.0  0.0  6.638745  4.048121  \n",
       "\n",
       "[372 rows x 50 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load input data\n",
    "\n",
    "# df_demand = pd.read_csv(input_data_filepath + '/' + 'total_daily_consumption_all_sectors_2019.csv')\n",
    "# df_net_storage = pd.read_csv(input_data_filepath + '/' + 'Daily Net Storage 2019.csv')\n",
    "\n",
    "# imports and exports: Note: units are MMcf, must be Bcf for model\n",
    "df_daily_imports = pd.read_csv(input_data_filepath + '/' + 'imports - daily pipeline and LNG smoothed - 2015-2019.csv').set_index('Date')\n",
    "df_daily_exports = pd.read_csv(input_data_filepath + '/' + 'exports - daily pipeline and LNG smoothed - 2015-2019.csv').set_index('Date')\n",
    "\n",
    "df_daily_imports = df_daily_imports/1000\n",
    "df_daily_exports = df_daily_exports/1000\n",
    "\n",
    "\n",
    "df_daily_demand_production = pd.read_csv(input_data_filepath + '/' + 'state_daily_production_demand_gulf_added_QPS_2015-2019-bcf.csv')\n",
    "\n",
    "df_daily_production_cols = df_daily_demand_production.pivot(index='date',columns='state abbrev',values='production (bcf)')\n",
    "df_daily_demand_cols = df_daily_demand_production.pivot(index='date',columns='state abbrev',values='demand (bcf)')\n",
    "\n",
    "# filter for last 7 days of 2018 for initial conditions plus 2019 data\n",
    "\n",
    "\n",
    "df_daily_imports_write = df_daily_imports.loc['2018-12-25':'2019-12-31'].reset_index()\n",
    "df_daily_exports_write = df_daily_exports.loc['2018-12-25':'2019-12-31'].reset_index()\n",
    "df_daily_production_write = df_daily_production_cols.loc['2018-12-25':'2019-12-31'].reset_index()\n",
    "df_daily_demand_write = df_daily_demand_cols.loc['2018-12-25':'2019-12-31'].reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# add zero nodes to demand data\n",
    "zero_nodes = ['ME_CN_E', 'NH_CN_E','VT_CN_E','NY_CN_E','MI_CN_E','MN_CN_W','ND_CN_W','MT_CN_W','ID_CN_W','WA_CN_W',\n",
    " 'TX_MX_NE','AZ_MX_NW','CA_MX_NW','TX_MX_SS','MX_NE','MX_NW','MX_IW','MX_CE','MX_SS','CN_E','CN_W']\n",
    "\n",
    "for node in zero_nodes:\n",
    "    df_daily_demand_write[node] = 0\n",
    "df_daily_demand_write.to_csv(input_data_filepath + '/' + 'check of zero node demands.csv')\n",
    "\n",
    "print('zero demand nodes manually added: MX, CN, border nodes')\n",
    "\n",
    "df_daily_imports_write\n",
    "df_daily_demand_write\n",
    "df_daily_production_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Tariff curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node to node flow tariffs and quantities added\n"
     ]
    }
   ],
   "source": [
    "#node-to-node flow tariffs\n",
    "nodal_flow_tariffs = pd.read_csv(input_data_filepath + '/' + 'PipelineTariff.csv',header=0,index_col = None)\n",
    "\n",
    "froms = list(nodal_flow_tariffs['from'])\n",
    "tos = list(nodal_flow_tariffs['to'])\n",
    "\n",
    "for f in froms:\n",
    "    new_f = f.replace('-','_')\n",
    "    i = froms.index(f)\n",
    "    froms[i] = new_f\n",
    "\n",
    "for t in tos:\n",
    "    new_t = t.replace('-','_')\n",
    "    i = tos.index(t)\n",
    "    tos[i] = new_t\n",
    "\n",
    "L = []\n",
    "for i in range(0,len(nodal_flow_tariffs)):\n",
    "    L.append(froms[i] + '_to_' + tos[i])\n",
    "\n",
    "nodal_flow_tariffs.index=L\n",
    "nodal_flow_tariffs = nodal_flow_tariffs.drop(columns=['to','from'])\n",
    "\n",
    "\n",
    "\n",
    "#node-to-node flow tariff quantities\n",
    "nodal_flow_tariffs_CAP = pd.read_csv(input_data_filepath + '/' + 'PipeTariffCurveQty_Daily_Downscaled.csv',header=0)\n",
    "\n",
    "froms = list(nodal_flow_tariffs_CAP['from'])\n",
    "tos = list(nodal_flow_tariffs_CAP['to'])\n",
    "\n",
    "for f in froms:\n",
    "    new_f = f.replace('-','_')\n",
    "    i = froms.index(f)\n",
    "    froms[i] = new_f\n",
    "\n",
    "for t in tos:\n",
    "    new_t = t.replace('-','_')\n",
    "    i = tos.index(t)\n",
    "    tos[i] = new_t\n",
    "    \n",
    "L = []\n",
    "for i in range(0,len(nodal_flow_tariffs_CAP)):\n",
    "    L.append(froms[i] + '_to_' + tos[i])\n",
    "\n",
    "nodal_flow_tariffs_CAP.index=L\n",
    "nodal_flow_tariffs_CAP = nodal_flow_tariffs_CAP.drop(columns=['to','from'])\n",
    "\n",
    "\n",
    "print('node to node flow tariffs and quantities added')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage (max withdrawal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>maxdeliv</th>\n",
       "      <th>maxdeliv (bcf)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AK</td>\n",
       "      <td>276830</td>\n",
       "      <td>0.276830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>2705000</td>\n",
       "      <td>2.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AR</td>\n",
       "      <td>212425</td>\n",
       "      <td>0.212425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA</td>\n",
       "      <td>8480000</td>\n",
       "      <td>8.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CO</td>\n",
       "      <td>1616595</td>\n",
       "      <td>1.616595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State  maxdeliv  maxdeliv (bcf)\n",
       "0    AK    276830        0.276830\n",
       "1    AL   2705000        2.705000\n",
       "2    AR    212425        0.212425\n",
       "3    CA   8480000        8.480000\n",
       "4    CO   1616595        1.616595"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_max_withdrawal = pd.read_csv(input_data_filepath + '/' + 'state_max_daily_storage.csv')\n",
    "\n",
    "df_max_withdrawal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to data file\n",
      "Producer set written\n",
      "node set written\n",
      "lines set written\n",
      "num_Qsteps set written\n",
      "num_FTariff_steps set written\n",
      "QPS step params written\n",
      "pipeline capacities param written\n",
      "pipeline tariffs and quantities param written\n",
      "demand params written\n",
      "imports written\n",
      "exports written\n",
      "Producers to node map written\n",
      "Line to node map written\n",
      "Complete: daily_gas_mod_data_6_base\n"
     ]
    }
   ],
   "source": [
    "print('writing to data file')\n",
    "\n",
    "######====== write data.dat file ======########\n",
    "with open(''+str(data_name)+'.dat', 'w') as f:\n",
    "\n",
    "  \n",
    "####### producer sets by type  \n",
    "\n",
    "    # Non-associated\n",
    "    f.write('set NA :=\\n')\n",
    "    \n",
    "    # pull relevant generators\n",
    "    for prod in range(0,len(df_QP_NA)):\n",
    "        unit_name = df_QP_NA.loc[prod,'producer']\n",
    "        f.write(unit_name + ' ')\n",
    "    f.write(';\\n\\n')        \n",
    "\n",
    "    print('Producer set written')\n",
    "\n",
    "\n",
    "######Set nodes, sources and sinks\n",
    "\n",
    "    # nodes\n",
    "    f.write('set nodes :=\\n')\n",
    "    for z in all_nodes:\n",
    "        name = z.replace('-','_')\n",
    "        f.write(name + ' ')\n",
    "    f.write(';\\n\\n')\n",
    "    \n",
    "    print('node set written')\n",
    "    \n",
    "    # lines\n",
    "    f.write('set lines :=\\n')\n",
    "    for z in lines:\n",
    "        f.write(z + ' ')\n",
    "    f.write(';\\n\\n')\n",
    "    \n",
    "    print('lines set written')\n",
    "    \n",
    "    \n",
    "###### Set of Qsteps\n",
    "    Qstep_set = [i for i in range(1,num_steps+1)]\n",
    "    f.write('set num_Qsteps :=\\n')\n",
    "    for i in Qstep_set:\n",
    "        f.write(str(i) + ' ')\n",
    "    f.write(';\\n\\n')\n",
    "    print('num_Qsteps set written')\n",
    "    \n",
    "\n",
    "###### Set of Ftariff steps (currently just the number of columns in the dataframe)\n",
    "    num_FTariff_steps = len(nodal_flow_tariffs_CAP.columns)\n",
    "    fstep_set = [i for i in range(1,num_FTariff_steps+1)]\n",
    "    \n",
    "    f.write('set num_FTariff_steps :=\\n')\n",
    "    for i in fstep_set:\n",
    "        f.write(str(i) + ' ')\n",
    "    f.write(';\\n\\n')\n",
    "    print('num_FTariff_steps set written')\n",
    "    \n",
    "\n",
    "####### simulation period and horizon\n",
    "    f.write('param SimDays := %d;' % SimDays)\n",
    "    f.write('\\n')\n",
    "    f.write('param HorizonDays := %d;' % HorizonDays)\n",
    "    f.write('\\n\\n')\n",
    "    \n",
    "###### slack costs\n",
    "    f.write('param pipeline_slack_cost := %d;' % pipeline_slack_cost)\n",
    "    f.write('\\n\\n')\n",
    "\n",
    "    f.write('param production_slack_cost := %d;' % production_slack_cost)\n",
    "    f.write('\\n\\n')\n",
    "\n",
    "    f.write('param storage_slack_cost := %d;' % storage_slack_cost)\n",
    "    f.write('\\n\\n')\n",
    "\n",
    "######=================================================########\n",
    "######              Producers                     ########\n",
    "######=================================================########\n",
    "    \n",
    "####### create parameter matrix for producers\n",
    "    f.write('param:' + '\\t' + 'Qstep' + '\\t'+ 'Pstep')\n",
    "    # for c in df_QP_NA.columns:\n",
    "    #     if c not in ['node', 'producer']:\n",
    "    #         f.write(c + '\\t')\n",
    "    f.write(':=\\n\\n')\n",
    "    for i in range(0,len(df_QP_NA)):    \n",
    "        \n",
    "        \n",
    "        for c in range(1,num_steps+1):\n",
    "            \n",
    "            unit_name = df_QP_NA.loc[i,'producer']\n",
    "            unit_name = unit_name.replace(' ','_')\n",
    "            unit_name = unit_name.replace('&','_')\n",
    "            unit_name = unit_name.replace('.','')\n",
    "            qn = 'Qstep' + str(c)\n",
    "            pn = 'Pstep' + str(c)\n",
    "            f.write(unit_name + '\\t' + str(c) + '\\t' + str(df_QP_NA.loc[i,qn]) + '\\t' + str(df_QP_NA.loc[i,pn]))  \n",
    "            f.write('\\n')\n",
    "            \n",
    "    f.write(';\\n\\n')     \n",
    "    \n",
    "    print('QPS step params written')\n",
    "    \n",
    "\n",
    "       \n",
    "\n",
    "######=================================================########\n",
    "######               Pipelines                       ########\n",
    "######=================================================########\n",
    "\n",
    "####### create parameter matrix for pipeline paths (source and sink connections)\n",
    "    f.write('param:' + '\\t' + 'FlowLim :=' + '\\n')\n",
    "    for z in lines:\n",
    "        f.write(z + '\\t' + str(df_pipelines.loc[z,'capacity']) + '\\n')\n",
    "    f.write(';\\n\\n')\n",
    "    \n",
    "    # create a separate csv file of lines and their capacities for the map\n",
    "    line_cap = []\n",
    "    for z in lines:\n",
    "        line_cap.append(df_pipelines.loc[z,'capacity'])\n",
    "        \n",
    "    df_linecap = pd.DataFrame({'line':lines,'capacity':line_cap})\n",
    "    df_linecap.to_csv(input_data_filepath + '/' + 'lines with capacities.csv',index=False)\n",
    "\n",
    "    print('pipeline capacities param written')\n",
    "    \n",
    "    ####### create parameter matrix for pipeline tariffs and capacities at each step\n",
    "\n",
    "    f.write('param:' + '\\t' + 'FTariff_CAP' + '\\t'+ 'FTariff')\n",
    "    f.write(':=\\n\\n')\n",
    "    \n",
    "    tariff_lst = list(nodal_flow_tariffs.index)\n",
    "    \n",
    "    for l in lines:\n",
    "        for c in range(1,num_FTariff_steps+1):\n",
    "            line_name = l\n",
    "            if l in tariff_lst:\n",
    "                f.write(line_name + '\\t' + str(c) + '\\t' + str(nodal_flow_tariffs_CAP.loc[l,'step'+str(c)]) + '\\t' + str(nodal_flow_tariffs.loc[l,'step'+str(c)]))\n",
    "                f.write('\\n')\n",
    "            else:\n",
    "                f.write(line_name + '\\t' + str(c) + '\\t' + str(0) + '\\t' + str(0))\n",
    "                f.write('\\n')\n",
    "    f.write(';\\n\\n')\n",
    "    \n",
    "    print('pipeline tariffs and quantities param written')\n",
    "    \n",
    "\n",
    "\n",
    "    ######=================================================########\n",
    "    ######               Storage                     ########\n",
    "    ######=================================================########\n",
    "\n",
    "\n",
    "    f.write('param:' + '\\t' + 'storage_max_withdrawal :=' + '\\n')\n",
    "    for z in all_nodes:\n",
    "        if z in df_max_withdrawal['State'].values:\n",
    "            max_with = df_max_withdrawal.loc[df_max_withdrawal['State'] == z]['maxdeliv (bcf)'].values[0]\n",
    "        else:\n",
    "            max_with = 0\n",
    "        f.write(z + '\\t' + str(max_with) + '\\n')\n",
    "    f.write(';\\n\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "    ######=================================================########\n",
    "    ######               Daily Time Series                 ########\n",
    "    ######=================================================########\n",
    "\n",
    "\n",
    "    ###### PRODUCTION #####\n",
    "    f.write('param:' + '\\t' + 'Sim_production :=' + '\\n')\n",
    "    for z in all_nodes:\n",
    "        for h in range(0,len(df_daily_production_write)):\n",
    "            new = z.replace('-','_')\n",
    "            if z in df_daily_production_write.columns:\n",
    "                daily_prod = df_daily_production_write.loc[h,z]\n",
    "                f.write(new + '\\t' + str(h+1) + '\\t' + str(daily_prod) + '\\n')\n",
    "            else:\n",
    "                f.write(new + '\\t' + str(h+1) + '\\t' + str(0.0) + '\\n')\n",
    "    f.write(';\\n\\n')\n",
    "    \n",
    "    ###### DEMAND #####\n",
    "\n",
    "    f.write('param:' + '\\t' + 'SimDemand :=' + '\\n')      \n",
    "    for z in all_nodes:\n",
    "        for h in range(0,len(df_daily_demand_write)):\n",
    "            new = z.replace('-','_')\n",
    "            if z in df_daily_demand_write.columns:\n",
    "                f.write(new + '\\t' + str(h+1) + '\\t' + str(df_daily_demand_write.loc[h,z]) + '\\n')\n",
    "            else:\n",
    "                f.write(new + '\\t' + str(h+1) + '\\t' + str(0.0) + '\\n')\n",
    "    f.write(';\\n\\n')\n",
    "    \n",
    "    print('demand params written')\n",
    "    \n",
    "    ###### STORAGE INJECTIONS #####\n",
    "    # f.write('param:' + '\\t' + 'Sim_storage_injection:=' + '\\n') \n",
    "    # for z in all_nodes:\n",
    "    #     for h in range(0,len(df_net_storage)):\n",
    "    #         new = z.replace('-','_')\n",
    "    #         if z in df_net_storage.columns:\n",
    "    #             storage_injection = abs(max(df_net_storage.loc[h,z],0))\n",
    "    #             f.write(new + '\\t' + str(h+1) + '\\t' + str(storage_injection) + '\\n')\n",
    "    #         else:\n",
    "    #             f.write(new + '\\t' + str(h+1) + '\\t' + str(0.0) + '\\n')\n",
    "    # f.write(';\\n\\n')            \n",
    "    # print('storage injections written')\n",
    "    \n",
    "    ###### STORAGE WITHDRAWALS #####\n",
    "    # f.write('param:' + '\\t' + 'Sim_storage_withdrawal:=' + '\\n') \n",
    "    # for z in all_nodes:\n",
    "    #     for h in range(0,len(df_net_storage)):\n",
    "    #         new = z.replace('-','_')\n",
    "    #         if z in df_net_storage.columns:\n",
    "    #             storage_withdrawal = abs(min(df_net_storage.loc[h,z],0))\n",
    "    #             f.write(new + '\\t' + str(h+1) + '\\t' + str(storage_withdrawal) + '\\n')\n",
    "    #         else:\n",
    "    #             f.write(new + '\\t' + str(h+1) + '\\t' + str(0.0) + '\\n')\n",
    "    # f.write(';\\n\\n')            \n",
    "    # print('storage withdrawals written')\n",
    "    \n",
    "    ###### IMPORTS #####\n",
    "    f.write('param:' + '\\t' + 'Sim_imports :=' + '\\n')\n",
    "    for z in all_nodes:\n",
    "        for h in range(0,len(df_daily_imports_write)):\n",
    "            new = z.replace('-','_')\n",
    "            if z in df_daily_imports_write.columns:\n",
    "                daily_import = df_daily_imports_write.loc[h,z]\n",
    "                f.write(new + '\\t' + str(h+1) + '\\t' + str(round(daily_import,3)) + '\\n')\n",
    "            else:\n",
    "                f.write(new + '\\t' + str(h+1) + '\\t' + str(0.0) + '\\n')\n",
    "    f.write(';\\n\\n')\n",
    "    print('imports written')\n",
    "    \n",
    "    ###### EXPORTS #####\n",
    "    f.write('param:' + '\\t' + 'Sim_exports :=' + '\\n')\n",
    "    for z in all_nodes:\n",
    "        for h in range(0,len(df_daily_exports_write)):\n",
    "            new = z.replace('-','_')\n",
    "            if z in df_daily_exports_write.columns:\n",
    "                daily_export = df_daily_exports_write.loc[h,z]\n",
    "                f.write(new + '\\t' + str(h+1) + '\\t' + str(round(daily_export,3)) + '\\n')\n",
    "            else:\n",
    "                f.write(new + '\\t' + str(h+1) + '\\t' + str(0.0) + '\\n')\n",
    "    f.write(';\\n\\n')\n",
    "    print('exports written')\n",
    "    \n",
    "    \n",
    "    ###### QPS TO NODE MAP DATA #####\n",
    "    \n",
    "    f.write('param QPS_to_node:')\n",
    "    f.write('\\n')\n",
    "    f.write('\\t' + '\\t')\n",
    "\n",
    "    for j in df_node_to_producer_map.columns:\n",
    "        if j!= 'name':\n",
    "            j_new = j.replace('-','_')\n",
    "            f.write(j_new + '\\t')\n",
    "    f.write(':=' + '\\n')\n",
    "    for i in range(0,len(df_node_to_producer_map)):   \n",
    "        for j in df_node_to_producer_map.columns:\n",
    "            f.write(str(df_node_to_producer_map.loc[i,j]) + '\\t')\n",
    "        f.write('\\n')\n",
    "    f.write(';\\n\\n')\n",
    "    \n",
    "    print('Producers to node map written')\n",
    "\n",
    "    ###### LINE TO NODE MAP DATA #####\n",
    "\n",
    "    f.write('param line_to_node:')\n",
    "    f.write('\\n')\n",
    "    f.write('\\t' + '\\t')\n",
    "\n",
    "    for j in df_line_to_node_map.columns:\n",
    "        if j!= 'line':\n",
    "            j_new = j.replace('-','_')\n",
    "            f.write(j_new + '\\t')\n",
    "    f.write(':=' + '\\n')\n",
    "    for l in lines:\n",
    "        f.write(str(l) + '\\t')\n",
    "        for j in df_line_to_node_map.columns:\n",
    "            f.write(str(df_line_to_node_map.loc[l,j]) + '\\t')\n",
    "        f.write('\\n')\n",
    "    f.write(';\\n\\n')\n",
    "    \n",
    "    print('Line to node map written')\n",
    "\n",
    "    \n",
    "\n",
    "print ('Complete:',data_name)\n",
    " # this opens the data file in spyder (can comment out if not needed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
